{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'japanize_matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-abfaed8d5fc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectrizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mContentVectorizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclustering\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClustarGenerator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnetwork_generater\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNetworkGenerater\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mcv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mContentVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Git/fragrance_words_analysis/src/network_generater.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnetworkx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrawing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnx_agraph\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwrite_dot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mjapanize_matplotlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mNetworkGenerater\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'japanize_matplotlib'"
     ]
    }
   ],
   "source": [
    "import MeCab\n",
    "import pandas as pd\n",
    "import re\n",
    "import itertools\n",
    "import math\n",
    "import numpy as np\n",
    "from src.vectrizer import ContentVectorizer\n",
    "from src.clustering import ClustarGenerator\n",
    "from src.network_generater import NetworkGenerater\n",
    "\n",
    "cv = ContentVectorizer()\n",
    "cg = ClustarGenerator()\n",
    "ng = NetworkGenerater()\n",
    "\n",
    "# ユーザー辞書を設定\n",
    "m_t = MeCab.Tagger('-d /usr/lib/x86_64-linux-gnu/mecab/dic/mecab-ipadic-neologd -u data/fragrance_user_dic.dic')\n",
    "\n",
    "# メタ付与する文章を読み込み\n",
    "article_df = pd.read_table('data/perfume_sentence.tsv')\n",
    "\n",
    "colname = ['surface', 'pos1', 'pos2', 'empty1', 'empty2', 'empty3', 'empty4', 'nomarization', 'category1', 'empty5', 'empty6', 'empty7', 'empty8', 'empty9', 'pos3', 'empty10', 'sign']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習済みベクトルを差し替える\n",
    "\n",
    "#model_path = \"model/awg_word2vec.model\"\n",
    "#cv.load_model(model_path)\n",
    "#cv.ginza.vocab.vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文章から抽出したワードのベクトルを作成する\n",
    "\n",
    "以下のようなテーブルを作成し、`特徴語リスト`の単語をそれぞれベクトル化する。\n",
    "\n",
    "|  記事文章  |  特徴語リスト  |\n",
    "| ---- | ---- |\n",
    "|  記事文章1  |  特徴語,特徴語,特徴語…  |\n",
    "|  記事文章2  |  特徴語,特徴語,特徴語…  |\n",
    "|  ...  |  ...  |\n",
    "\n",
    "ベクトル化できなかった単語は`empty_w`リストに入れておく。使用する学習モデルの性能確認用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 辞書から特定のカテゴリーに属するワードを抽出する\n",
    "feature = [] # 記事ごとに特徴語リストを格納\n",
    "word_class = [] # 記事の文章から抽出した一意のワードを格納\n",
    "wlstr_ = [] # 一時変数\n",
    "empty_w = [] # ベクトル化できない単語を格納（ゼロベクトルのリスト）\n",
    "\n",
    "for i, sentence in enumerate(article_df['description']):\n",
    "    parsed_s = m_t.parse(sentence).replace('\\t',',').split('\\n')\n",
    "    parsed_results = pd.Series(parsed_s).str.split(',').tolist()\n",
    "    df_raw = pd.DataFrame(parsed_results, columns = colname)\n",
    "    # 'category1'に「香り」か「印象」属性が登録されている単語をDataFarameに登録\n",
    "    f_df = df_raw[(df_raw['category1'] == '香り') | (df_raw['category1'] =='印象')]\n",
    "\n",
    "    # 単語と香りor印象属性を抽出する\n",
    "    f_m = []\n",
    "    attr_dic = {}\n",
    "    for s,c in zip(f_df['surface'],f_df['category1']):\n",
    "        doc,vector = cv.vectorize(s)\n",
    "        attr_dic = {'word':doc,'category':c}\n",
    "        if vector.all():\n",
    "            if str(doc) not in wlstr_:\n",
    "                word_class.append(doc)\n",
    "                wlstr_.append(str(doc))\n",
    "            f_m.append(attr_dic)\n",
    "        else:\n",
    "            empty_w.append(doc)\n",
    "    feature.append(f_m)\n",
    "    \n",
    "# 出力\n",
    "print('ベクトル化できなかった単語:',empty_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 特徴語の類似度テーブルを作成する\n",
    "\n",
    "|  特徴語A  |  特徴語B  |  類似度(スコア)  |\n",
    "| ---- | ---- | ---- |\n",
    "|  特徴語  |  特徴語  |  0.356119  |\n",
    "|  特徴語  |  特徴語  |  0.464299  |\n",
    "|  ...  |  ...  |  ...  |\n",
    "\n",
    "特徴語同士がどれくらい似ているか比較します。1に近いほど2つの特徴語が似ていることになります。  \n",
    "ただし、学習モデルに登録されていないワードは抜き出せません。  \n",
    "例：「ダマスクローズ」というワードは学習モデルに存在しないため、「ダマスク」と「ローズ」それぞれのベクトルの平均値が「ダマスクローズ」のベクトルとして出力されます。  \n",
    "　　「ダマスク」というワードが存在しない場合は「ローズ」のベクトルが出力されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 特徴語の組み合わせを全パターン作成する\n",
    "meta_parts_pairs = []\n",
    "for pair in itertools.combinations(word_class, 2):\n",
    "    meta_parts_pairs.append(pair)\n",
    "    \n",
    "# 特徴語の類似度（ベクトル同士の類似度）を計算する\n",
    "score = []\n",
    "for doc1,doc2 in meta_parts_pairs:\n",
    "    score.append(doc1.similarity(doc2))\n",
    "    \n",
    "# 上記の結果をテーブルに格納\n",
    "target_df = pd.DataFrame(meta_parts_pairs)\n",
    "target_df['2']=score\n",
    "\n",
    "# 出力\n",
    "target_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# クラスタリング\n",
    "k-meansで特徴語をクラスタリングします。  \n",
    "参考：[k-means法を理解する](https://qiita.com/g-k/items/0d5d22a12a4507ecbf11)\n",
    "\n",
    "- `n_cluster`: クラスタ数を設定できます。  \n",
    "- `cluster_result`: {クラスタID:特徴語リスト}な辞書です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorlist = np.array([w.vector for w in word_class])\n",
    "\n",
    "# クラスタ数の設定\n",
    "n_cluster = 10\n",
    "\n",
    "cluster_result = cg.generate(title_list=word_class, \n",
    "            vector_list=vectorlist, \n",
    "            vector_size=100, \n",
    "            n_cluster = n_cluster)\n",
    "\n",
    "print(cluster_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# クラスタごとのネットワークを作成\n",
    "- `cluster_result_dict`: {クラスタID:特徴語同士の距離テーブル}な辞書を作成します。グラフ構造を作成するために使います。\n",
    "- `cluster_df['2']>0.5`でスコアのしきい値を設定します。デフォルトでは0.5以下のスコアは表示されません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_word = cluster_result[0]\n",
    "\n",
    "cluster_result_dict = {}\n",
    "for class_id , cluster_word in cluster_result.items():\n",
    "    meta_parts_pairs = []\n",
    "    for pair in itertools.combinations(cluster_word, 2):\n",
    "        meta_parts_pairs.append(pair)\n",
    "\n",
    "    score = []\n",
    "    for doc1,doc2 in meta_parts_pairs:\n",
    "        score.append(round(doc1.similarity(doc2),3))\n",
    "\n",
    "    cluster_df = pd.DataFrame(meta_parts_pairs)\n",
    "    cluster_df['2']=score\n",
    "\n",
    "    cluster_result_dict[class_id]=cluster_df[cluster_df['2']>0.5].sort_values('2',ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ネットワーク可視化\n",
    "クラスタごとに特徴語のネットワークを作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for clusterid, cluster in cluster_result_dict.items():\n",
    "    print('Cluster ID: 'clusterid)\n",
    "    ng.network(cluster.values.tolist(), 200, 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
